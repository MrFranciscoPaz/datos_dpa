{
  "metadata": {
    "name": "Note converted from Jupyter_2F8YD8EWB",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n\n%sh\nwget   data_2010_1_1.parquet  https://github.com/MrFranciscoPaz/datos_dpa/raw/master/data_2010_1_1.parquet\nwget   data_2010_1_2.parquet  https://github.com/MrFranciscoPaz/datos_dpa/raw/master/data_2010_1_2.parquet\nwget   data_2010_1_3.parquet  https://github.com/MrFranciscoPaz/datos_dpa/raw/master/data_2010_1_3.parquet\nwget   data_2010_1_4.parquet  https://github.com/MrFranciscoPaz/datos_dpa/raw/master/data_2010_1_4.parquet\nwget   data_2010_1_5.parquet  https://github.com/MrFranciscoPaz/datos_dpa/raw/master/data_2010_1_5.parquet\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n\n%pyspark\n\npath \u003d \u0027./data_2010_1_*.parquet\u0027\ndf \u003d spark.read.parquet(path)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n\n%pyspark\ndf2 \u003d df"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n\n%pyspark\nfrom pyspark.sql import functions as F\n\n# Corregimos valores nulos\ndef as_null(x):\n    return F.when(F.col(x).isin(\u0027N/A\u0027, \u0027nan\u0027, \u0027NaN\u0027, \u0027n/a\u0027, \u0027Na\u0027, \u0027\u0027), None).otherwise(F.col(x))\n\n\ncols_str \u003d [item[0] for item in df2.dtypes if item[1].startswith(\u0027string\u0027)]\nfor i in cols_str:\n    df2 \u003d df2.withColumn(i, as_null(i))\n\n\n\n# Corregimos el tipo de las variables\ntimeFmt \u003d \"yyyy-MM-dd\u0027T\u0027HH:mm:ss.SSS\"\ncreated_date \u003d F.unix_timestamp(\u0027created_date\u0027, format\u003dtimeFmt)\nclosed_date \u003d F.unix_timestamp(\u0027closed_date\u0027, format\u003dtimeFmt)\nresolution_action_updated_date \u003d F.unix_timestamp(\u0027resolution_action_updated_date\u0027, format\u003dtimeFmt)\ndue_date \u003d F.unix_timestamp(\u0027due_date\u0027, format\u003dtimeFmt)\n\ndf2 \u003d df2.withColumn(\"created_date_timestamp\", created_date)\ndf2 \u003d df2.withColumn(\"closed_date_timestamp\", closed_date)\ndf2 \u003d df2.withColumn(\"resolution_action_updated_date_timestamp\", resolution_action_updated_date)\ndf2 \u003d df2.withColumn(\"due_date_timestamp\", due_date)\n\n\ncreated_date \u003d F.from_unixtime(F.unix_timestamp(df2[\u0027created_date\u0027],\"yyyy-MM-dd\u0027T\u0027HH:mm:ss.SSS\"),\"yyyy-MM-dd\").alias(\"created_date\").cast(\"date\")\nclosed_date \u003d F.from_unixtime(F.unix_timestamp(df2[\u0027closed_date\u0027],\"yyyy-MM-dd\u0027T\u0027HH:mm:ss.SSS\"),\"yyyy-MM-dd\").alias(\"closed_date\").cast(\"date\")\nresolution_action_updated_date \u003d F.from_unixtime(F.unix_timestamp(df2[\u0027resolution_action_updated_date\u0027],\"yyyy-MM-dd\u0027T\u0027HH:mm:ss.SSS\"),\"yyyy-MM-dd\").alias(\"resolution_action_updated_date\").cast(\"date\")\ndue_date \u003d  F.from_unixtime(F.unix_timestamp(df2[\u0027due_date\u0027],\"yyyy-MM-dd\u0027T\u0027HH:mm:ss.SSS\"),\"yyyy-MM-dd\").alias(\"due_date\").cast(\"date\")\n\ndf2 \u003d df2.withColumn(\"created_date\", created_date)\ndf2 \u003d df2.withColumn(\"closed_date\", closed_date)\ndf2 \u003d df2.withColumn(\"resolution_action_updated_date\", resolution_action_updated_date)\ndf2 \u003d df2.withColumn(\"due_date\", due_date)\n\n\n\n# Cambiamos las variables string a lowercase\ncols_str \u003d [item[0] for item in df2.dtypes if item[1].startswith(\u0027string\u0027)]\nfor c in cols_str:\n    col \u003d F.lower(df2[c]).alias(c)\n    df2 \u003d df2.withColumn(c, col)\n\n\n\n# Eliminamos registros repetidos\ndf2.dropDuplicates()\n\n# Eliminamos registros que tengan puros valores nulos\ndf2.dropna(how\u003d\u0027all\u0027)\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n\n%pyspark\ntype(df2)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndf2.columns"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndf2.select(\u0027created_date\u0027).distinct().rdd.map(lambda r: r[0]).collect()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ncd \u003d df2.groupBy(\"created_date\").count().orderBy(\u0027created_date\u0027)\ncd.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ncd \u003d cd.withColumnRenamed(\"count\", \"number_cases\")\ncond \u003d [df2[\u0027created_date\u0027] \u003d\u003d cd[\u0027created_date\u0027]]\ndf3 \u003d df2.join(cd, cond, \u0027left\u0027).drop(cd[\u0027created_date\u0027])\ndf3.printSchema()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndf.count()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndf2.count()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ntype(df3)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndf3.groupBy(\"number_cases\").count().show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ncd.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ncd2 \u003d cd.withColumn(\u0027created_date_1\u0027, F.date_add(cd[\u0027created_date\u0027], 1))\ncd2.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndf3.count()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ncd2 \u003d cd2.withColumnRenamed(\"number_cases\", \"numeber_cases_1days_ago\")\ncd2.show()\ncond \u003d [df3[\u0027created_date\u0027] \u003d\u003d cd2[\u0027created_date_1\u0027]]\ndf4 \u003d df3.join(cd2, cond, \u0027left\u0027).drop(cd2[\u0027created_date\u0027])\ndf4 \u003d df4.drop(cd2[\u0027created_date_1\u0027])\ndf4.printSchema()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndf4.groupBy(\"numeber_cases_1days_ago\").count().show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndf4.count()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndf4.groupBy(\"created_date\").agg()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}